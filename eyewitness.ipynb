{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eyewitness.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0b0t-x/eyewitness/blob/master/eyewitness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ7nBiXZwFm8",
        "colab_type": "text"
      },
      "source": [
        "# Face Recognition Using Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlgpcUgVp6Jt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9403418c-09c6-44ad-823d-091b0040662d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9RMYcPawFm9",
        "colab_type": "text"
      },
      "source": [
        "##Get Data\n",
        "We will understand the siamese network by building the face recognition model. The objective of our network is to understand whether two faces are similar or dissimilar. We use AT & T's the Database of Faces which can be downloaded from here (https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html)\n",
        "\n",
        "Once you have downloaded and extracted the archive, you can see the folders like s1, s2 up to s40 as shown here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogUA4B73qZMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "12fa434f-5514-4275-d1f2-32c483e829f0"
      },
      "source": [
        "!wget http://www.cl.cam.ac.uk/Research/DTG/attarchive/pub/data/att_faces.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-12 15:32:44--  http://www.cl.cam.ac.uk/Research/DTG/attarchive/pub/data/att_faces.zip\n",
            "Resolving www.cl.cam.ac.uk (www.cl.cam.ac.uk)... 128.232.0.20, 2a05:b400:110::80:14\n",
            "Connecting to www.cl.cam.ac.uk (www.cl.cam.ac.uk)|128.232.0.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.cl.cam.ac.uk/Research/DTG/attarchive/pub/data/att_faces.zip [following]\n",
            "--2019-05-12 15:32:45--  https://www.cl.cam.ac.uk/Research/DTG/attarchive/pub/data/att_faces.zip\n",
            "Connecting to www.cl.cam.ac.uk (www.cl.cam.ac.uk)|128.232.0.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att_faces.zip [following]\n",
            "--2019-05-12 15:32:45--  https://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att_faces.zip\n",
            "Reusing existing connection to www.cl.cam.ac.uk:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3769022 (3.6M) [application/zip]\n",
            "Saving to: ‘att_faces.zip’\n",
            "\n",
            "att_faces.zip       100%[===================>]   3.59M  4.81MB/s    in 0.7s    \n",
            "\n",
            "2019-05-12 15:32:46 (4.81 MB/s) - ‘att_faces.zip’ saved [3769022/3769022]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKHXa7KnruwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir 'gdrive/My Drive/documents/work/youngee'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgqTtC4hqkCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip att_faces.zip -d 'gdrive/My Drive/documents/work/youngee'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwFbtJoFw6oS",
        "colab_type": "text"
      },
      "source": [
        "Each of these folders has 10 different images of a single person taken from various angles. For an instance, let us open folder s1. As you can see, there are 10 different images of a single person:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M65n6EL5r05n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1b01785b-24a2-42aa-f4b6-ca6a47ee8728"
      },
      "source": [
        "!ls 'gdrive/My Drive/documents/work/youngee'"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README\ts11  s14  s17  s2   s22  s25  s28  s30\ts33  s36  s39  s5  s8\n",
            "s1\ts12  s15  s18  s20  s23  s26  s29  s31\ts34  s37  s4   s6  s9\n",
            "s10\ts13  s16  s19  s21  s24  s27  s3   s32\ts35  s38  s40  s7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDUW6cI-tJ-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = 'gdrive/My Drive/documents/work/youngee'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0TCkv5Cxijw",
        "colab_type": "text"
      },
      "source": [
        "##Import libraries\n",
        "First, we will import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGkgJ_MksVQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCQNNND0wFnM",
        "colab_type": "text"
      },
      "source": [
        "##Read image as numpy array\n",
        "Now, we define a function for reading our input image. The function read_image takes input as an image and returns the numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "656arWptsdnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image(filename, byteorder='>'):\n",
        "    \n",
        "    #first we read the image, as a raw file to the buffer\n",
        "    with open(filename, 'rb') as f:\n",
        "        buffer = f.read()\n",
        "    \n",
        "    #using regex, we extract the header, width, height and maxval of the image\n",
        "    header, width, height, maxval = re.search(\n",
        "        b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
        "    \n",
        "    #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array\n",
        "    return np.frombuffer(buffer,\n",
        "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
        "                            count=int(width)*int(height),\n",
        "                            offset=len(header)\n",
        "                            ).reshape((int(height), int(width)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGTmXP9WwFnQ",
        "colab_type": "text"
      },
      "source": [
        "##Visualize Sample image\n",
        "For an example, Let us open one image,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuB_Yjd-tuNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_path = \"{}/s1/1.pgm\".format(DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rmf5P-2spB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "8571486f-f0b3-4d1d-ac96-daac51106035"
      },
      "source": [
        "Image.open(image_path)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABwCAAAAAC26kjJAAAZDUlEQVR4nC2ZS6+u2XWV55hzrrXe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\ninmRzLYwJ/0/S7JsNfhWRLwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.PpmImagePlugin.PpmImageFile image mode=L size=92x112 at 0x7F4256569518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkhzdzqcwFnV",
        "colab_type": "text"
      },
      "source": [
        "##Read image as an array\n",
        "When we feed this image to our read_image function, it will return as the numpy array,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KeUXyBbtF3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e43b6560-0b62-448f-f320-a1c2ae6383d7"
      },
      "source": [
        "img = read_image(image_path)\n",
        "img.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112, 92)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAEhFmf7xDaA",
        "colab_type": "text"
      },
      "source": [
        "##Get Image Pair\n",
        "As we know that siamese networks require inputs as a pair along with the label, we have to create our data in such a way. So we will take two images randomly from the same folder and mark it as a genuine pair and we will take a single image from two different folders and mark them as an imposite pair. A sample data is shown in the below figure, as you can notice a genuine pair has images of the same person and imposite pair has images of a different person."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kanVx_PEetEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "data = glob.glob(DATA_PATH+\"/*/\")\n",
        "\n",
        "size = 2\n",
        "total_sample_size = len(data)\n",
        "\n",
        "\n",
        "def get_data(size, total_sample_size):\n",
        "    #read the image\n",
        "    image = read_image(DATA_PATH+'/s' + str(1) + '/' + str(1) + '.pgm', 'rw+')\n",
        "    \n",
        "    #reduce the size\n",
        "    image = image[::size, ::size]\n",
        "    \n",
        "    #get the new size\n",
        "    dim1 = image.shape[0]\n",
        "    dim2 = image.shape[1]\n",
        "\n",
        "    count = 0\n",
        "    \n",
        "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
        "    x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2]) # 2 is for pairs\n",
        "    y_genuine = np.zeros([total_sample_size, 1])\n",
        "    \n",
        "    for i in range(40):\n",
        "        for j in range(int(total_sample_size/40)):\n",
        "            ind1 = 0\n",
        "            ind2 = 0\n",
        "            \n",
        "            #read images from same directory (genuine pair)\n",
        "            while ind1 == ind2:\n",
        "                ind1 = np.random.randint(10)\n",
        "                ind2 = np.random.randint(10)\n",
        "            \n",
        "            # read the two images\n",
        "            img1 = read_image(DATA_PATH+'/s' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+')\n",
        "            img2 = read_image(DATA_PATH+'/s' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+')\n",
        "            \n",
        "            #reduce the size\n",
        "            img1 = img1[::size, ::size]\n",
        "            img2 = img2[::size, ::size]\n",
        "            \n",
        "            #store the images to the initialized numpy array\n",
        "            x_geuine_pair[count, 0, 0, :, :] = img1\n",
        "            x_geuine_pair[count, 1, 0, :, :] = img2\n",
        "            \n",
        "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
        "            y_genuine[count] = 1\n",
        "            count += 1\n",
        "\n",
        "    count = 0\n",
        "    x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
        "    y_imposite = np.zeros([total_sample_size, 1])\n",
        "    \n",
        "    for i in range(int(total_sample_size/10)):\n",
        "        for j in range(10):\n",
        "            \n",
        "            #read images from different directory (imposite pair)\n",
        "            while True:\n",
        "                ind1 = np.random.randint(40)\n",
        "                ind2 = np.random.randint(40)\n",
        "                if ind1 != ind2:\n",
        "                    break\n",
        "                    \n",
        "            img1 = read_image(DATA_PATH+'/s' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
        "            img2 = read_image(DATA_PATH+'/s' + str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
        "\n",
        "            img1 = img1[::size, ::size]\n",
        "            img2 = img2[::size, ::size]\n",
        "\n",
        "            x_imposite_pair[count, 0, 0, :, :] = img1\n",
        "            x_imposite_pair[count, 1, 0, :, :] = img2\n",
        "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
        "            y_imposite[count] = 0\n",
        "            count += 1\n",
        "            \n",
        "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
        "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
        "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB5oiftnwFnd",
        "colab_type": "text"
      },
      "source": [
        "Now, we define another function get_data for generating our data. As we know, for the Siamese network, data should be in the form of pairs (genuine and imposite) with a binary label.\n",
        "\n",
        "First, we read the images (img1, img2) from the same directory and store them in the x_genuine_pair array and assign y_genuine to 1. Next, we read the images (img1, img2) from the different directory and store them in the x_imposite pair and assign y_imposite to 0.\n",
        "\n",
        "Finally, we concatenate both x_genuine_pair, x_imposite to X and y_genuine, y_imposite to Y:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVsWsX05tYzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = 2\n",
        "total_sample_size = 10000\n",
        "\n",
        "\n",
        "def get_data(size, total_sample_size):\n",
        "    #read the image\n",
        "    image = read_image(DATA_PATH+'/s' + str(1) + '/' + str(1) + '.pgm', 'rw+')\n",
        "    \n",
        "    #reduce the size\n",
        "    image = image[::size, ::size]\n",
        "    \n",
        "    #get the new size\n",
        "    dim1 = image.shape[0]\n",
        "    dim2 = image.shape[1]\n",
        "\n",
        "    count = 0\n",
        "    \n",
        "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
        "    x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2]) # 2 is for pairs\n",
        "    y_genuine = np.zeros([total_sample_size, 1])\n",
        "    \n",
        "    for i in range(40):\n",
        "        for j in range(int(total_sample_size/40)):\n",
        "            ind1 = 0\n",
        "            ind2 = 0\n",
        "            \n",
        "            #read images from same directory (genuine pair)\n",
        "            while ind1 == ind2:\n",
        "                ind1 = np.random.randint(10)\n",
        "                ind2 = np.random.randint(10)\n",
        "            \n",
        "            # read the two images\n",
        "            img1 = read_image(DATA_PATH+'/s' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+')\n",
        "            img2 = read_image(DATA_PATH+'/s' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+')\n",
        "            \n",
        "            #reduce the size\n",
        "            img1 = img1[::size, ::size]\n",
        "            img2 = img2[::size, ::size]\n",
        "            \n",
        "            #store the images to the initialized numpy array\n",
        "            x_geuine_pair[count, 0, 0, :, :] = img1\n",
        "            x_geuine_pair[count, 1, 0, :, :] = img2\n",
        "            \n",
        "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
        "            y_genuine[count] = 1\n",
        "            count += 1\n",
        "\n",
        "    count = 0\n",
        "    x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
        "    y_imposite = np.zeros([total_sample_size, 1])\n",
        "    \n",
        "    for i in range(int(total_sample_size/10)):\n",
        "        for j in range(10):\n",
        "            \n",
        "            #read images from different directory (imposite pair)\n",
        "            while True:\n",
        "                ind1 = np.random.randint(40)\n",
        "                ind2 = np.random.randint(40)\n",
        "                if ind1 != ind2:\n",
        "                    break\n",
        "                    \n",
        "            img1 = read_image(DATA_PATH+'/s' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
        "            img2 = read_image(DATA_PATH+'/s' + str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
        "\n",
        "            img1 = img1[::size, ::size]\n",
        "            img2 = img2[::size, ::size]\n",
        "\n",
        "            x_imposite_pair[count, 0, 0, :, :] = img1\n",
        "            x_imposite_pair[count, 1, 0, :, :] = img2\n",
        "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
        "            y_imposite[count] = 0\n",
        "            count += 1\n",
        "            \n",
        "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
        "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
        "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw5sHuZ5xMBW",
        "colab_type": "text"
      },
      "source": [
        "##Train network\n",
        "Once we have our data as pairs along with their labels, we train our siamese network. From the image pair, we feed one image to the network A and another image to the network B. The role of these two networks is only to extract the feature vectors. So, we use two convolution layers with relu activations for extracting the features. Once we have learned the feature, we feed the resultant feature vector from both of the networks to the energy function which measures the similarity, we use Euclidean distance as our energy function. So, we train our network by feeding the image pair to learn the semantic similarity between them. Now, we will see this step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG_YmMbKwFni",
        "colab_type": "text"
      },
      "source": [
        "Now, we generate our data and check our data size. As you can see we have 20,000 data points, out of these 10,000 are genuine pairs and 10,000 are imposite pairs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLlxy0KLwFnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y = get_data(size, total_sample_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERfLHBK1wFno",
        "colab_type": "code",
        "outputId": "8186bc46-5391-4231-cc69-99a6387be688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 2, 1, 56, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-zfkpEpqwFns",
        "colab_type": "code",
        "outputId": "2cadeb0e-86b7-4058-a9a7-9416d1effe99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9x70ki7wFny",
        "colab_type": "text"
      },
      "source": [
        "Next, we split our data for training and testing with 75% training and 25% testing proportions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kkiWpIywFny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWgYs-wcwFn4",
        "colab_type": "text"
      },
      "source": [
        "Now that, we have successfully generated our data, we build our siamese network. First, we define the base network which is basically a convolutional network used for feature extraction. We build two convolutional layers with rectified linear unit (ReLU) activations and max pooling followed by flat layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhmBHJ3-wFn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_base_network(input_shape):\n",
        "    \n",
        "    seq = Sequential()\n",
        "    \n",
        "    nb_filter = [6, 12]\n",
        "    kernel_size = 3\n",
        "    \n",
        "    \n",
        "    #convolutional layer 1\n",
        "    seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape,\n",
        "                          border_mode='valid', dim_ordering='th'))\n",
        "    seq.add(Activation('relu'))\n",
        "    seq.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "    seq.add(Dropout(.25))\n",
        "    \n",
        "    #convolutional layer 2\n",
        "    seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th'))\n",
        "    seq.add(Activation('relu'))\n",
        "    seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) \n",
        "    seq.add(Dropout(.25))\n",
        "\n",
        "    #flatten \n",
        "    seq.add(Flatten())\n",
        "    seq.add(Dense(128, activation='relu'))\n",
        "    seq.add(Dropout(0.1))\n",
        "    seq.add(Dense(50, activation='relu'))\n",
        "    return seq\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82R1tY7lwFn_",
        "colab_type": "text"
      },
      "source": [
        "Next, we feed the image pair, to the base network, which will return the embeddings that is, feature vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-YYpNvQwFoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = x_train.shape[2:]\n",
        "img_a = Input(shape=input_dim)\n",
        "img_b = Input(shape=input_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZo_pDNhwFoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d863a61f-b0c4-409b-8c8c-85ad4d7a3644"
      },
      "source": [
        "base_network = build_base_network(input_dim)\n",
        "feat_vecs_a = base_network(img_a)\n",
        "feat_vecs_b = base_network(img_b)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(6, (3, 3), input_shape=(1, 56, 46..., padding=\"valid\", data_format=\"channels_first\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), padding=\"valid\", data_format=\"channels_first\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QWxPecEwFoP",
        "colab_type": "text"
      },
      "source": [
        "These feat_vecs_a and feat_vecs_b are the feature vectors of our image pair. Next, we feed this feature vectors to the energy function to compute the distance between them, we use Euclidean distance as our energy function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7L3AT_KwFoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ-M63tawFoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batAMimcwFoZ",
        "colab_type": "text"
      },
      "source": [
        " Now, we set the epoch length to 13 and we use RMS prop for optimization and define our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp23KMEXwFoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 13\n",
        "rms = RMSprop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFIg6b2uwFoc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c52f5430-ccff-4757-c442-8244427dc1cb"
      },
      "source": [
        "model = Model(input=[img_a, img_b], output=distance)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"la...)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02asKPdTwFoe",
        "colab_type": "text"
      },
      "source": [
        "Next, we define our loss function as contrastive_loss function and compile the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za4Y77sEwFof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0Ym-0MSwFoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=contrastive_loss, optimizer=rms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP2eNyXIwFol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_1 = x_train[:, 0]\n",
        "img2 = x_train[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR0PhmYowFop",
        "colab_type": "code",
        "outputId": "f98a00b4-40f2-4117-cf6b-e93a042587f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "model.fit([img_1, img2], y_train, validation_split=.25,\n",
        "          batch_size=128, verbose=2, nb_epoch=epochs)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 11250 samples, validate on 3750 samples\n",
            "Epoch 1/13\n",
            " - 44s - loss: 0.2435 - val_loss: 0.2869\n",
            "Epoch 2/13\n",
            " - 44s - loss: 0.1652 - val_loss: 0.2704\n",
            "Epoch 3/13\n",
            " - 44s - loss: 0.1265 - val_loss: 0.1953\n",
            "Epoch 4/13\n",
            "Epoch 5/13\n",
            " - 44s - loss: 0.0836 - val_loss: 0.1140\n",
            "Epoch 6/13\n",
            " - 44s - loss: 0.0681 - val_loss: 0.0802\n",
            "Epoch 7/13\n",
            " - 44s - loss: 0.0573 - val_loss: 0.0678\n",
            "Epoch 8/13\n",
            " - 44s - loss: 0.0513 - val_loss: 0.0513\n",
            "Epoch 9/13\n",
            " - 44s - loss: 0.0454 - val_loss: 0.0520\n",
            "Epoch 10/13\n",
            " - 44s - loss: 0.0402 - val_loss: 0.0390\n",
            "Epoch 11/13\n",
            " - 44s - loss: 0.0369 - val_loss: 0.0384\n",
            "Epoch 12/13\n",
            " - 44s - loss: 0.0331 - val_loss: 0.0292\n",
            "Epoch 13/13\n",
            " - 44s - loss: 0.0313 - val_loss: 0.0350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f425068df28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LAho1oewFot",
        "colab_type": "text"
      },
      "source": [
        "Now, we make predictions with test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od2F9BBrwFou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict([x_test[:, 0], x_test[:, 1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BVN77jFwFow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(predictions, labels):\n",
        "    return labels[predictions.ravel() < 0.5].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUEPmr03wFo2",
        "colab_type": "text"
      },
      "source": [
        "Finally, we check our model accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9zRc75pkwFo3",
        "colab_type": "code",
        "outputId": "d0858b4c-8519-4ab2-92ea-55c5225a6a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "compute_accuracy(pred, y_test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9300673148840688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROF1RtY47jWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a8b0471-0304-4956-e645-0bc301dcb705"
      },
      "source": [
        "x_test[0, 0].shape\n",
        "model.predict([x_test[:1, 0], x_test[:1, 1]])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5666597]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33SbYzouR-lX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5b96da58-523b-45d0-a080-74cd4dae1f2b"
      },
      "source": [
        "pred"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5666596 ],\n",
              "       [0.01803328],\n",
              "       [0.03845473],\n",
              "       ...,\n",
              "       [1.0385535 ],\n",
              "       [0.10629258],\n",
              "       [0.8508386 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLh1yZPRTNJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "29928af7-d42e-4c9a-9420-d581d0cba791"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       ...,\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    }
  ]
}